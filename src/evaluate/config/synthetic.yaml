# Evaluation Configuration for Synthetic Multimodal Dataset
#
# This configuration evaluates VLMs on the synthetic quantum computing dataset
# generated by the synthetic_data pipeline. The dataset uses HuggingFace format
# with three question types:
#
#   - function_completion: Code stub completion (like Qiskit HumanEval normal)
#   - code_generation: Full code generation from NL (like Qiskit HumanEval hard)
#   - qa: Text-based Q&A with similarity metrics
#
# Code types use unit test execution for evaluation, QA uses ROUGE-L/BLEU.

# Model Configuration
model:
  base_url: "${MODEL_BASE_URL}"
  api_key: "${API_KEY}"
  model_name: "${MODEL_NAME}"
  max_tokens: 4096
  temperature: 0.0 # Greedy decoding for deterministic evaluation
  timeout: 300.0
  is_vlm: true

# Dataset Configuration
dataset:
  type: "synthetic"
  # Path to HuggingFace dataset directory (outputs/final from synthetic pipeline)
  path: "/Users/samuel/Developer/avante/unifei/tcc/quantum-assistant/outputs/final"
  # Directory containing images (if not embedded in dataset)
  images_dir: "/Users/samuel/Developer/avante/unifei/tcc/quantum-assistant/outputs/images"
  # Dataset split to evaluate
  split: "test"
  # Limit samples for testing (null for all)
  max_samples: null
  # Filter to text-only samples (no images) - useful for LLMs without vision
  text_only: false

# Metrics Configuration
metrics:
  # Number of solutions to generate per task (for Pass@k)
  num_samples_per_task: 1
  # K values for Pass@k computation
  k_values: [1]
  # Code execution timeout in seconds
  execution_timeout: 60
  # Maximum concurrent API requests
  max_concurrent: 16

  # System prompt configuration
  # Options: "qiskit_humaneval", "qiskit_humaneval_minimal", "generic", "custom", or null
  system_prompt_type: "qiskit_humaneval"
  custom_system_prompt: null

  verify_canonical: false

# Output Configuration
output:
  results_file: null
  results_dir: "outputs/evaluate/synthetic"
  auto_filename: true
