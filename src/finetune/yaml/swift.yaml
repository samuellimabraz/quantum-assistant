# =============================================================================
# Swift Fine-tuning Configuration for Quantum Assistant
# Model: Qwen3-VL-2B-Instruct with LoRA
# =============================================================================

# -----------------------------------------------------------------------------
# Model Configuration
# -----------------------------------------------------------------------------
model: Qwen/Qwen3-VL-2B-Instruct
model_name: Qwen3-VL-2B-Instruct-r16-rslora-bf16-tuned
model_author: samuellimabraz

# -----------------------------------------------------------------------------
# Dataset Configuration
# -----------------------------------------------------------------------------
dataset:
  - ./outputs/quantum-assistant/train.jsonl
val_dataset:
  - ./outputs/quantum-assistant/validation.jsonl
load_from_cache_file: true
data_seed: 42

# -----------------------------------------------------------------------------
# Model Precision & Attention
# -----------------------------------------------------------------------------
torch_dtype: bfloat16
bf16: true
fp16: false
attn_impl: flash_attn  # Options: flash_attn, sdpa, eager

# -----------------------------------------------------------------------------
# Tokenization & Sequence Settings
# -----------------------------------------------------------------------------
padding_side: left
padding_free: true
lazy_tokenize: true
# max_length: 8192
max_new_tokens: 4096
max_pixels: 1003520  # 1280 * 28 * 28
temperature: 0.0

# -----------------------------------------------------------------------------
# LoRA Configuration
# -----------------------------------------------------------------------------
train_type: lora
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
target_modules: all-linear
init_weights: 'true'
use_rslora: true

# Freeze settings
freeze_llm: false
freeze_vit: true
freeze_aligner: true

# -----------------------------------------------------------------------------
# Training Configuration
# -----------------------------------------------------------------------------
output_dir: ./outputs/train/Qwen3-VL-2B-Instruct-r16-rslora-bf16-tuned

# Optimization
learning_rate: 2e-4
lr_scheduler_type: cosine
optim: adamw_torch
weight_decay: 0.01

# Batch & Accumulation
per_device_train_batch_size: 16
gradient_accumulation_steps: 2
torch_empty_cache_steps: 50

# Epochs & Steps
num_train_epochs: 1
warmup_steps: 10
warmup_ratio: 0.05

# Memory Optimization
gradient_checkpointing: true
# vit_gradient_checkpointing: false

# -----------------------------------------------------------------------------
# Evaluation Configuration
# -----------------------------------------------------------------------------
per_device_eval_batch_size: 8
eval_accumulation_steps: 4
eval_strategy: steps
eval_steps: 20
metric_for_best_model: eval_loss
load_best_model_at_end: true
fp16_full_eval: true

# -----------------------------------------------------------------------------
# Saving Configuration
# -----------------------------------------------------------------------------
save_strategy: best
save_steps: 20
save_total_limit: 5

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
logging_first_step: true
logging_steps: 5
logging_strategy: steps
report_to:
  - wandb
  - tensorboard

# Project & Run naming
project: quantum-assistant
run_name: Qwen3-VL-2B-Instruct-r16-rslora-bf16-tuned

# -----------------------------------------------------------------------------
# Data Loading
# -----------------------------------------------------------------------------
dataloader_num_workers: 8
dataset_num_proc: 8
remove_unused_columns: false

# -----------------------------------------------------------------------------
# HuggingFace Hub Configuration
# -----------------------------------------------------------------------------
use_hf: true
push_to_hub: false
hub_private_repo: true
hub_model_id: samuellimabraz/Qwen3-VL-2B-Instruct-r16-rslora-bf16-tuned

# Early stopping
early_stop_interval: 3

